{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from datetime import datetime as dt\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "import calendar\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, \\\n",
    "                                  date_format\n",
    "from datetime import datetime as dt\n",
    "\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = config.get(\"AWS\", \"AWS_ACCESS_KEY_ID\")\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = config.get\\\n",
    "                                      (\"AWS\", \"AWS_SECRET_ACCESS_KEY\")\n",
    "\n",
    "# os.environ['AWS_ACCESS_KEY_ID']=''\n",
    "# os.environ['AWS_SECRET_ACCESS_KEY']=''\n",
    "\n",
    "song_input_data = config.get(\"S3\", \"SONG_DATA\")\n",
    "log_input_data = config.get(\"S3\", \"LOG_DATA\")\n",
    "output_data = config.get(\"S3\", \"OUTPUT_DATA\")\n",
    "\n",
    "# song_input_data = 's3a://udacity-dend/song-data/A/*/*/TRAAAAK128F9318786.json'\n",
    "# log_input_data  = 's3a://udacity-dend/log-data/2018/11/2018-11-01-events.json'\n",
    "# output_data = 's3a://ankit-rawat'\n",
    "\n",
    "\n",
    "def create_spark_session():\n",
    "      \n",
    "    \"\"\" \n",
    "    The function Create a Apache Spark session to process the data\n",
    "      \n",
    "    Paramters :N/A\n",
    "    \n",
    "    Output: An Apache Spark session.    \n",
    "    \"\"\"\n",
    "    \n",
    "    print('Creating Spark session at', dt.now())\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    print('Spark session is completed at', dt.now())\n",
    "    return spark\n",
    "\n",
    "\n",
    "def process_song_data(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    The function processes the song json files from S3 and \n",
    "    creates Parquet file of song and artist.\n",
    "    \n",
    "    Paramters :\n",
    "    \n",
    "    spark : sparkSession\n",
    "    \n",
    "    input_data : input files\n",
    "    \n",
    "    output_data : output directory on S3 where parquet files\n",
    "                  are to be created.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    print('Processing Song data at', dt.now())\n",
    "   \n",
    "    # get filepath to song data file\n",
    "    song_data = input_data\n",
    "    \n",
    "    # read song data file\n",
    "    df = spark.read.json(song_data)\n",
    "\n",
    "    # extract columns to create songs table\n",
    "    songs_table = df['song_id', 'title', 'artist_id', 'year', 'duration']\n",
    "    \n",
    "    # write songs table to parquet files partitioned by year and artist\n",
    "    songs_table.write.partitionBy('year', 'artist_id').\\\n",
    "      parquet(os.path.join(output_data,'songs.parquet'),'overwrite')\n",
    "    \n",
    "    print('Song Parquet file completed at', dt.now())\n",
    "\n",
    "    # extract columns to create artists table\n",
    "    artists_table = df['artist_id', 'artist_name', 'artist_location',\\\n",
    "     'artist_latitude', 'artist_longitude']\n",
    "   \n",
    "    \n",
    "    # write artists table to parquet files\n",
    "    artists_table.write.parquet(os.path.join(output_data,'artists.parquet'),'overwrite')\n",
    "    print('Artist Parquet file completed at', dt.now())\n",
    "\n",
    "\n",
    "def process_log_data(spark, input_data, output_data):\n",
    "    \n",
    "    \"\"\"\n",
    "    Load JSON input data from input_data path,\n",
    "    process the data and write the output to AWS.\n",
    "        \n",
    "    Paramters:\n",
    "    * spark         -- Spark session object\n",
    "    * input_data    -- input files\n",
    "    * output_data   -- output directory on S3 where parquet files\n",
    "                        are to be created.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    print('Processing log data at', dt.now())\n",
    "    \n",
    "    # get filepath to log data file\n",
    "    log_data = input_data\n",
    "\n",
    "    # read log data file\n",
    "    df = spark.read.json(log_data)\n",
    "    \n",
    "    # filter by actions for song plays\n",
    "    df = df.filter(df['page']=='NextSong')\n",
    "    \n",
    "    df.printSchema()\n",
    "\n",
    "    # extract columns for users table    \n",
    "    users_table = df['userId', 'firstName', 'lastName', 'gender', 'level']\n",
    "    \n",
    "    # write users table to parquet files\n",
    "    users_table.write.parquet('users.parquet', 'overwrite')\n",
    "    \n",
    "    print('Creating time parquet file at', dt.now())\n",
    "    \n",
    "\n",
    "    # create timestamp column from original timestamp column\n",
    "    # get_timestamp = udf(lambda a: str(int(int(a)/1000)))\n",
    "    # df = df.withColumn('timestamp', get_timestamp(df.ts))\n",
    "    \n",
    "    # get_timestamp = udf(lambda x: str(int(int(x) / 1000)))\n",
    "    # df = df.withColumn(\"timestamp\", get_timestamp(df.ts))\n",
    "    \n",
    "    # create datetime column from original timestamp column\n",
    "    get_datetime = udf(lambda x: str(dt.fromtimestamp(int(x) / 1000.0)))\n",
    "    df = df.withColumn(\"datetime\", get_datetime(df.ts))\n",
    "    \n",
    "    # extract columns to create time table\n",
    "    time_table = df.select(\n",
    "    'datetime',\n",
    "    hour('datetime').alias('hour'),\n",
    "    dayofmonth('datetime').alias('day'),\n",
    "    weekofyear('datetime').alias('week'),\n",
    "    month('datetime').alias('month'),\n",
    "    year('datetime').alias('year'),\n",
    "    date_format('datetime', 'F').alias('weekday'))\n",
    "    \n",
    "    time_table.show(5)\n",
    "    time_table.printSchema()\n",
    "     \n",
    "    # write time table to parquet files partitioned by year and month\n",
    "    time_table.write.partitionBy('year', 'month').\\\n",
    "    parquet(os.path.join(output_data,'time.parquet'), 'overwrite')\n",
    "    \n",
    "    print('Completed time parquet file at', dt.now())\n",
    "    \n",
    "    # read in song data to use for songplays table\n",
    "    song_df = spark.read.parquet(\"songs.parquet\")\n",
    "    print('Song dataframe')\n",
    "    song_df.printSchema()\n",
    "    song_df.select('title','artist_id').show(10)\n",
    "    print('Dataframe')\n",
    "    df.select('song','artist').show(10)\n",
    "    \n",
    "    # extract columns from joined song and log datasets to create songplays table \n",
    "    print('Creating songplay parquet file at', dt.now())\n",
    "    df = df.join(song_df, ((song_df.title == df.song)), 'inner')\n",
    "    songplays_table = df['datetime', 'userId', 'level', 'song_id', \\\n",
    "    'artist_id', 'sessionId', 'location', 'userAgent']\n",
    "    \n",
    "    songplays_table.show(5)\n",
    "    \n",
    "    # write songplays table to parquet files partitioned by year and month\n",
    "    songplays_table.write.parquet(\\\n",
    "                    os.path.join(output_data, 'songplays.parquet')\\\n",
    "                                , 'overwrite')\n",
    "    print('Completed songplay parquet file at', dt.now())\n",
    "    print('Completed ETL process at', dt.now())\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "     Main function which creates Spark session and \n",
    "     then call the process functions to load \n",
    "     and process the song and log data        \n",
    "     Keyword arguments:\n",
    "        * N/A\n",
    "    \n",
    "    \"\"\"\n",
    "    print('Starting ETL at', dt.now())\n",
    "    spark = create_spark_session()\n",
    "    # song_input_data =\n",
    "    # \"s3a://udacity-dend/song-data/A/*/*/TRAAAAK128F9318786.json\"\n",
    "    # log_input_data = \n",
    "    #  \"s3a://udacity-dend/log-data/*/*/*/TRAAAAK128F9318786.json\"\n",
    "    # output_data = \"\"\n",
    "    \n",
    "    process_song_data(spark, song_input_data, output_data)    \n",
    "    process_log_data(spark, log_input_data, output_data)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
